{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from furl import Path\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import least_squares\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Install and connect trains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\nUsage:   \r\n  pip install [options] <requirement specifier> [package-index-options] ...\r\n  pip install [options] -r <requirements file> [package-index-options] ...\r\n  pip install [options] [-e] <vcs project url> ...\r\n  pip install [options] [-e] <local project path> ...\r\n  pip install [options] <archive url/path> ...\r\n\r\nno such option: --yes\r\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "!pip install --yes --prefix {sys.prefix} trains\n",
    "from trains import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 3\n",
    "FONT_SIZE = 14"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "curve-fitting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_minimize(fun):\n",
    "    \"\"\"\n",
    "    In order to \n",
    "    :param fun: predictor function\n",
    "    :return: function calculated output - function predicted output \n",
    "    \"\"\"\n",
    "    return lambda w, x, y: fun(w, x) - y\n",
    "\n",
    "\n",
    "def log_predictor(x: list, t: int, eps: int=EPS)->float:\n",
    "    \"\"\"\n",
    "    Logarithmic regression model.\n",
    "    :param x: Vector of the model weights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :param eps: Epsilon, for numerical stability\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return np.log(t + eps) * x[0] + x[1]\n",
    "\n",
    "\n",
    "def log_log_predictor(x, t, eps=EPS)->float:\n",
    "    \"\"\"\n",
    "    Log-Logarithmic regression model.\n",
    "    :param x: Vector of the model weights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :param eps: Epsilon, for numerical stability\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return np.log(np.log(t + eps)) * x[0] + x[1]\n",
    "\n",
    "\n",
    "def log_log_log_predictor(x, t, eps=EPS)->float:\n",
    "    \"\"\"\n",
    "    Log-Log-Logarithmic regression model.\n",
    "    :param x: Vector of the model wights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :param eps: Epsilon, for numerical stability\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return np.log(np.log(np.log(t + eps))) * x[0] + x[1]\n",
    "\n",
    "\n",
    "def pareto_predictor(x, t, eps=EPS)->float:\n",
    "    \"\"\"\n",
    "    Pareto CDF used a regression model.\n",
    "    :param x: Vector of the model weights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :param eps: Epsilon, for numerical stability\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return 1 - np.power(1 / (t + eps), x[0])\n",
    "\n",
    "\n",
    "def generelised_pareto_predictor(x, t, eps=EPS)->float:\n",
    "    \"\"\"\n",
    "    Generalisation of the Pareto CDF as a regression model, to 3 learnable parameters, instead of 1.\n",
    "    :param x: Vector of the model weights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :param eps: Epsilon, for numerical stability\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return x[0] - x[1] * np.power(1 / (t + eps), x[2])\n",
    "\n",
    "\n",
    "def arm_predictor(x, t)->float:\n",
    "    \"\"\"\n",
    "    Asymproric regression model\n",
    "    :param x: Vector of the model weights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return x[2] - x[0] * np.exp(-x[1] * t)\n",
    "\n",
    "\n",
    "def poly_log_predictor(t, x, eps=EPS)->float:\n",
    "    \"\"\"\n",
    "    Polynome on the explanatory variable after a log transformation.\n",
    "    :param x: Vector of the model wights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :param eps: Epsilon, for numerical stability\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return np.log(eps + t * x[0]) * x[1] + np.log(eps + t * x[2]) * x[3]\n",
    "\n",
    "\n",
    "def multi_log_predictor(x, t, eps=EPS) -> float:\n",
    "    \"\"\"\n",
    "    A model combining logarithmic regression and log-logarithmic regressoin.\n",
    "    :param x: Vector of the model weights.\n",
    "    :param t: The model explanatory variable.\n",
    "    :param eps: Epsilon, for numerical stability\n",
    "    :return: Prediction of the model given (t;x)\n",
    "    \"\"\"\n",
    "    return np.log(eps + t * x[0]) * x[1] + np.log(np.log(eps + t * x[0])) * x[1]\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "curve-fitting stuff"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def to_minimize(fun: function)->function:\n",
    "\t\"\"\"\n",
    "\tTakes a function 'fun' of two variables.\n",
    "\tReturns a function that takes those two variables and the wanted output\n",
    "\t('ground truth', for example) of 'fun'and calculates\n",
    "\tthe residual between 'fun' output and the wanted output.\n",
    "\t:param fun: a function of two variables.\n",
    "\t:return: function calculating the residual between\n",
    "\t'fun' output and the wanted output.\n",
    "\t\"\"\"\n",
    "\treturn lambda w, x, y: fun(w, x) - y\n",
    "\n",
    "def fit_and_predict(\n",
    "    predictor: function,\n",
    "    train_sumples: np.ndarray,\n",
    "    t: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    x0: np.ndarray = np.array([0.4, 0.01, 0.4, 0.1]),\n",
    "):\n",
    "    \"\"\"\n",
    "\tFit the function parameters based of the first 'num_of_predicitons'\n",
    "\tand predict the values (usually mAP for object detection tasks) for all of the data.\n",
    "\t:param predictor: The function predicting the data.\n",
    "\t:param t: The explanatory variable.\n",
    "\t:param y: Ground truth of the response variable.\n",
    "\t:param x0: Initial guess of the predictor parameters.\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "    fun = to_minimize(predictor)\n",
    "    t_train, y_train = t[train_sumples], y[train_sumples]\n",
    "    predictor_res = least_squares(\n",
    "        fun,\n",
    "        x0,\n",
    "        args=(t_train, y_train),\n",
    "        bounds=[(-np.inf, -np.inf, -np.inf, -np.inf), (50, np.inf, np.inf, np.inf)],\n",
    "    )\n",
    "    y_predictor = predictor(predictor_res.x, t)\n",
    "    return y_predictor, predictor_res\n",
    "\n",
    "\n",
    "def get_train_sumples(t: np.ndarray, y:np.ndarray, indicies_array:np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Sample the data vectors t and y to create a train set.\n",
    "    :param t: The planatory variable observation.\n",
    "    :param y: The Response variable observation.\n",
    "    :param indicies_array: State which datapoints to take as train set for the model.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    t_train = t[indicies_array]\n",
    "    y_train = y[indicies_array]\n",
    "    return t_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Plot the diminishing returns prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(\n",
    "        predictor, train_samples: np.ndarray, t: np.ndarray, y: np.ndarray\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit the function parameters based on \n",
    "    the first 'num_of_predictions' and predict the values\n",
    "    (usually mAP for object detection tasks)\n",
    "    for all of the data.   \n",
    "    :param predictor: The function prediction the values (mAP)\n",
    "    :param train_samples: Which of the training samples\n",
    "    will be used to train the predictor.\n",
    "    :param t: The training set size (Explanatory variable).\n",
    "    :param y: The values (Response variable).\n",
    "    :param x0: Initial guess of the predictor parameters.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    x0=np.array([0.4, 0.01, 0.4, 0.1])\n",
    "    fun = to_minimize(predictor)\n",
    "    t_train, y_train = t[train_samples], y[train_samples]\n",
    "    trained_predictor = least_squares(\n",
    "        fun, x0, args=(t_train, y_train))\n",
    "    y_predictions = predictor(trained_predictor.x, t)\n",
    "    return y_predictions, trained_predictor\n",
    "\n",
    "    \n",
    "def plot_predictions(\n",
    "    predictor,\n",
    "    csv_filepath: Path,\n",
    "    network_names: Sequence[str]=[\"MobileNet v2\", \"ResNet 101\"],\n",
    "    predictor_name: str=\"Logarithmic predictor\",\n",
    "    train_samples=np.arange(4),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict and plot the diminishing returns dynamics.\n",
    "    :param predictor: model predicting the diminishing returns dymanics.\n",
    "    :param csv_filepath: path to a csv path. In this csv file, column #1 should be the subset size,\n",
    "    column #2 the ground truth of the first detector and coloum #3 - the ground truth of the second\n",
    "    detector, if any.\n",
    "    :param network_names: Names of models used for detection.\n",
    "    :param predictor_name: Name of the diminishing returns dynamic predictor.\n",
    "    :param train_samples: Which samples of the data should be used to tweak the predictor parameters.\n",
    "    \"\"\"\n",
    "    colors = [\"g\", \"b\", \"c\", \"m\", \"y\", \"r\", \"b\"]\n",
    "    colors = colors[: len(network_names)]\n",
    "    r_sqr_array = []\n",
    "    csv_file = pd.read_csv(csv_filepath)\n",
    "    t_train = np.array(csv_file[csv_file.columns[1]])\n",
    "    mobilenet_train = np.array(csv_file[csv_file.columns[2]])\n",
    "    resnet_train = np.array(csv_file[csv_file.columns[3]])\n",
    "    plt.rc(\"font\", size=FONT_SIZE)  # controls default text sizes\n",
    "    plt.rc(\"axes\", titlesize=FONT_SIZE)  # fontsize of the axes title\n",
    "    plt.rc(\"axes\", labelsize=FONT_SIZE)  # fontsize of the x and y labels\n",
    "    plt.rc(\"xtick\", labelsize=FONT_SIZE)  # fontsize of the tick labels\n",
    "    plt.rc(\"ytick\", labelsize=FONT_SIZE)  # fontsize of the tick labels\n",
    "    plt.rc(\"legend\", fontsize=FONT_SIZE)  # legend fontsize\n",
    "    plt.rc(\"figure\", titlesize=FONT_SIZE)  # fontsize of the figure title\n",
    "    for y_train, network_name, color in zip(\n",
    "        [mobilenet_train, resnet_train], network_names, colors\n",
    "    ):\n",
    "        y_predicted, fitted_predictor = fit_and_predict(\n",
    "            predictor=predictor, train_samples=train_samples, t=t_train,\n",
    "            y=y_train\n",
    "        )\n",
    "        plt.plot(\n",
    "            t_train,\n",
    "            y_predicted,\n",
    "            color=color,\n",
    "            label=f\"Predictions, {network_name}\".format(),\n",
    "        )\n",
    "        plt.plot(\n",
    "            t_train,\n",
    "            y_train,\n",
    "            color=color,\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Ground truth, {network_name}\".format(),\n",
    "        )\n",
    "        plt.plot(t_train[train_samples], y_train[train_samples], \"o\", color=color)\n",
    "        r_sqr_array.append(r2_score(y_train, y_predicted))\n",
    "\n",
    "    plt.xlabel(\"Data volume\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    r_sqr_string = [\n",
    "        f\"{network_name}: {str(r_squr)[:5]}\"\n",
    "        for network_name, r_squr in zip(network_names, r_sqr_array)\n",
    "    ]\n",
    "    r_sqr_string = \"; \".join(r_sqr_string)\n",
    "    plt.title(predictor_name + \". R sqr: \" + \"\\n\" + r_sqr_string)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{predictor_name}.png\", format=\"png\", dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filepath = \"path/to/diminishing returns results.csv\"\n",
    "csv_file = pd.read_csv(csv_filepath)\n",
    "t_train = np.array(csv_file[csv_file.columns[1]])\n",
    "y_train = np.array(csv_file[csv_file.columns[2]])\n",
    "\n",
    "plot_predictions(\n",
    "predictor=log_predictor,\n",
    "csv_filepath=csv_filepath,\n",
    "train_samples=np.arange(4),\n",
    "predictor_name=\"Logarithmic regression model, BDD Dataset \",\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
